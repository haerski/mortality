```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  fig.width = 6, fig.height = 6, dpi = 150, fig.path = "figures/time-"
)
```

# Time-based models
Here we will look at what happens when time-dependent data is added. At the moment, we only use deaths per day per zipcode

## Data wrangling
This is going to be mostly the same as `baseline_models.Rmd`. Refer to that file for more details.
Note that here we're not extracting the AEs.
```{r}
library(tidyverse)
library(tidymodels)
library(feather)
# library(arrow)
library(magrittr)
library(skimr)
library(lubridate)
library(timetk)
library(modeltime)
library(glue)
library(slider)
per <- read_feather("data/simulation_data/all_persons.feather")

clients <-
  per %>%
  group_by(client) %>%
  summarize(
    zip3 = first(zip3),
    size = n(),
    volume = sum(FaceAmt),
    avg_qx = mean(qx),
    avg_age = mean(Age),
    per_male = sum(Sex == "Male") / size,
    per_blue_collar = sum(collar == "blue") / size,
    expected = sum(qx * FaceAmt)
  )

zip_data <-
  read_feather("data/data.feather") %>%
  mutate(
    density = POP / AREALAND,
    AREALAND = NULL,
    AREA = NULL,
    HU = NULL,
    vaccinated = NULL,
    per_lib = NULL,
    per_green = NULL,
    per_other = NULL,
    per_rep = NULL,
    unempl_2020 = NULL,
    deaths_covid = NULL,
    deaths_all = NULL
  ) %>%
  rename(
    unemp = unempl_2019,
    hes_uns = hes_unsure,
    str_hes = strong_hes,
    income = Median_Household_Income_2019
  )

clients %<>%
  inner_join(zip_data, by = "zip3") %>%
  drop_na()
```

Now we have to be careful...

We take daily covid deaths and make them weekly.
Also, look at weekly deaths instead of deaths to date
```{r}
deaths <-
  read_feather("data/deaths_zip3.feather") %>%
  mutate(date = ceiling_date(date, unit = "week")) %>%
  group_by(zip3, date) %>%
  summarize(totdeaths = max(deaths)) %>%
  mutate(zip_deaths = totdeaths - lag(totdeaths, default = 0)) %>%
  select(-totdeaths) %>%
  ungroup()
```

Let's see what it looks like in Atlanta and LA. We normalize by population.
```{r atl_la_covid}
deaths %>%
  left_join(zip_data, by = "zip3") %>%
  filter(zip3 %in% c("303", "900")) %>%
  ggplot(aes(x = date, color = zip3)) +
  geom_line(aes(y = zip_deaths / POP))
```

One thing to note, there is no death data for zipcodes 202, 204, 753, 772 (which is fine, since there is no zip_data for those either).

Next we look at the deaths only
```{r}
per %<>%
  drop_na() %>%
  select(-month)
```

2019 didn't have 53 weeks, so the deaths on week 53 in 2019 will be changed to death in week 1 of 2020.

Now back to the 53 week business
```{r}
wk53 <-
  per %>%
  filter(year == 2019, week == 53) %>%
  mutate(year = 2020, week = 1)
per %<>%
  rows_update(wk53, by = c("client", "participant"))
```

The tibble `per` contains all the people that die and their deathweek. Everyone should die only once now! Let's convert the week and year into the last date of that week.

```{r}
per %<>%
  nest_by(week, year) %>%
  mutate(date = ceiling_date(ymd(glue("{year}-01-01")) + weeks(week - 1), unit = "week")) %>%
  ungroup() %>%
  select(-week, -year) %>%
  unnest(cols = c(data))
```



Next, we need some kind of a rolling count for AE. Looks like the package `slider` might help.
I want actual claims per week for each client.
We note that there are 4 clients that won't have any deaths
```{r}
no_deaths <-
  clients %>%
  anti_join(per, by = "client") %>%
  select(client) %>%
  mutate(date = ceiling_date(ymd("2019-01-01"), unit = "week"), claims = 0)
```

We compute face amount per week for each client. This number is 0 if the client has no deaths that week.
```{r}
weekly_claims <-
  per %>%
  group_by(date, client) %>%
  summarize(claims = sum(FaceAmt), .groups = "drop") %>%
  bind_rows(no_deaths) %>%
  complete(date, client, fill = list(claims = 0))
```

We now have 65,369 rows, which is 131 weeks * 499 clients.

Let's merge everything.
```{r}
weekly_data <-
  clients %>%
  left_join(weekly_claims, by = c("client")) %>%
  relocate(date) %>%
  relocate(claims, .after = zip3) %>%
  left_join(deaths, by = c("date", "zip3")) %>%
  relocate(zip_deaths, .after = claims) %>%
  mutate(zip_deaths = replace_na(zip_deaths, 0), ae = claims / (expected / 52.18))
```

We add a rolling AE number. We will smooth the actual claims number of each week by taking a weighted average of actual claims in the 13 weeks prior. The weights come from a Gaussian distribution...
The function `sliding_smoother` takes a vector and outputs a vector of smoothed values.

The below picture show what the smoother does for claims of client 7. We also show a 3 month mean AE
```{r}
smoother <- function(x) { weighted.mean(x, dnorm(seq(-1, 0, length.out = length(x)), sd = 0.33)) }
sliding_smoother <-
  slidify(smoother, .period = 13, .align = "right")

sliding_mean <-
  slidify(mean, .period = 13, .align = "right")


weekly_data %>%
  filter(client == 7) %>%
  ggplot(aes(x = date)) +
  # geom_line(aes(y = sliding_mean(claims)), color = "red") +
  # geom_line(aes(y = smooth_vec(claims, period = 13)), color = "blue") +
  geom_line(aes(y = sliding_smoother(ae)), color = "magenta") +
  geom_line(aes(y = sliding_mean(ae)), color = "red") +
  geom_line(aes(y = ae), linetype = 2)
```

We add a column called `smoothed_ae` and `smoothed_deaths`.
```{r}
weekly_data <-
  weekly_data %>%
  group_by(client) %>%
  mutate(smoothed_ae = sliding_smoother(ae), smoothed_deaths = sliding_smoother(zip_deaths), .before = size) %>%
  drop_na()
```

We plot the average AE for each week. This is pretty crazy...
```{r}
weekly_data %>%
  ungroup() %>%
  group_by(date) %>%
  summarize(avg_ae = mean(smoothed_ae), sd = sd(smoothed_ae)) %>%
  ggplot(aes(x = date, y = avg_ae)) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed")
  # geom_errorbar(aes(ymin = avg_ae - sd, ymax = avg_ae + sd))
```

Weekly normalized deaths in the zipcodes of our clients... I had hoped that this curve looked the same as the AE curve, but I guess it doesn't.
```{r}
weekly_data %>%
  ungroup() %>%
  group_by(date) %>%
  summarize(avg_deaths = mean(zip_deaths / POP)) %>%
  ggplot(aes(x = date, y = avg_deaths)) + geom_line()
```

Let compute the 25th percentile AE of each week. We will pick the threshold for `adverse` based on this.
```{r}
weekly_data %>%
  ungroup() %>%
  group_by(date) %>%
  summarize(
      `12.5` = quantile(smoothed_ae, 0.125),
      `25` = quantile(smoothed_ae, 0.25),
      `50` = quantile(smoothed_ae, 0.50)
  ) %>%
  pivot_longer(-date, names_to = "pth", values_to = "smoothed_ae") %>%
  ggplot(aes(x = date, y = smoothed_ae, color = pth)) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = "dashed")
```

Maybe there is a better number to look at than smoothed ae. Let's shrink smoothed ae based on the log(Volume * average qx). This gives us some kind of a measure of client size and mortality.
```{r}
client_shrinkage <-
  weekly_data %>%
  summarize(dep_var = first(volume * avg_qx)) %>%
  mutate(shrinkage = rescale(log(dep_var), to = c(0.3, 1)))

ggplot(client_shrinkage, aes(x = dep_var)) + geom_density() + scale_x_log10()

processed_data <-
  weekly_data %>%
  left_join(client_shrinkage, by = "client") %>%
  ungroup() %>%
  mutate(shrunk_ae = smoothed_ae * shrinkage, .after = smoothed_ae)

processed_data %>%
  group_by(date) %>%
  summarize(avg_ae = mean(shrunk_ae)) %>%
  ggplot(aes(date, avg_ae)) + geom_line()


processed_data %>%
  ungroup() %>%
  group_by(date) %>%
  summarize(
      `12.5` = quantile(shrunk_ae, 0.125),
      `25` = quantile(shrunk_ae, 0.25),
      `50` = quantile(shrunk_ae, 0.50)
  ) %>%
  pivot_longer(-date, names_to = "pth", values_to = "value") %>%
  ggplot(aes(x = date, y = value, color = pth)) +
  geom_line() +
  geom_hline(yintercept = 2.5, linetype = "dashed")
```



We will choose "smoothed, shrunk 3 month AE" > 2.5 as "Adverse".
```{r}
processed_data <-
  processed_data %>%
  ungroup() %>%
  mutate(class = factor(if_else(shrunk_ae > 2.5, "Adverse", "Not adverse"), levels = c("Adverse", "Not adverse")), .after = shrunk_ae)
```


How many clients are adverse each week?
```{r}
processed_data %>%
  group_by(date) %>%
  summarize(prop_adverse = sum(class == "Adverse") / n()) %>%
  ggplot(aes(x = date, y = prop_adverse)) + geom_line()
```


# Modeling (with tidyverts)

## Case study: Jan 1st 2021

### ARIMA on zip_deaths + random forest

We will travel back in time. We will attempt to predict six months worth of "averse" variables for all clients.
```{r}
library(tsibble)
library(fable)

data_tsibble <-
  processed_data
  # mutate(yw = yearweek(date), .before = date, .keep = "unused")

# splits <-
#   data_tsibble %>%
#   filter(date >= "2020-03-15") %>%
#   nest_by(date) %>%
#   sliding_index(index = date,
#                 lookback = weeks(12),
#                 assess_stop = weeks(26))

# split <- splits %>% pluck(1, 3)
# train <- training(split) %>% unnest(data)
# test <- testing(split) %>% unnest(data)
train <-
  processed_data %>%
  filter(date <= "2021-01-01")

test <-
  processed_data %>%
  filter(date > "2021-01-01" & date <= "2021-06-01")
```

Here we will manually forecast future `zip_deaths` using a fully default ARIMA forecaster. Maybe we should actually predict a smoothed version of `zip_deaths`???
We plot total deaths vs total forecasted deaths (in the zip codes of our clients). It's not great but it's something...
```{r cache = TRUE}
forecast <-
  train %>%
  filter(date >= "2020-03-15") %>%
  as_tsibble(index = date, key = client) %>%
  model(arima = ARIMA(zip_deaths)) %>%
  forecast(h = "6 months")

foo <-
  forecast %>%
  index_by(date) %>%
  summarize(fc_deaths = sum(.mean))

bar <-
  test %>%
  as_tsibble(key = client, index = date) %>%
  index_by(date) %>%
  summarize(true_deaths = sum(zip_deaths))

foo %>%
  left_join(bar, by = "date") %>%
  ggplot(aes(x = date)) + geom_line(aes(y = fc_deaths), color = "red") + geom_line(aes(y = true_deaths))
```

We will train a forest on all the data prior to Jan 1st 2021
```{r}
forest_spec <-
  rand_forest(trees = 1000) %>%
  set_engine("ranger", num.threads = 8, seed = 123456789) %>%
  set_mode("classification")

forest_recipe <-
  recipe(class ~ ., data = processed_data) %>%
  step_rm(client, zip3, claims, smoothed_ae, shrunk_ae, shrinkage, dep_var, ae, smoothed_deaths) %>%
  step_zv(all_predictors())

forest_wf <-
  workflow() %>%
  add_recipe(forest_recipe) %>%
  add_model(forest_spec)

trained_wf <-
  forest_wf %>%
  fit(train)
```

We substitute `zip_deaths` by the forecasted version of `zip_deaths` and predict.
We plot the `roc_auc` of our predictions.
```{r}
forecasted_test <-
  forecast %>%
  as_tibble() %>%
  select(client, date, .mean) %>%
  right_join(test, by = c("client", "date")) %>%
  select(-zip_deaths) %>%
  rename(zip_deaths = .mean)

trained_wf %>%
  predict(forecasted_test, type = "prob") %>%
  bind_cols(test) %>%
  group_by(date) %>%
  summarize(roc_auc = roc_auc_vec(class, .pred_Adverse)) %>%
  ggplot(aes(x = date, y = roc_auc)) + geom_line()
```

How well can we possibly do? Let's use the true deaths, and compare with our forecasted ones.
Doesn't look too bad. Maybe a random forest is not the best model ?
```{r}
test %>%
  bind_cols(predict(trained_wf, forecasted_test)) %>%
  rename(pred_forecast = .pred_class) %>%
  bind_cols(predict(trained_wf, test)) %>%
  rename(pred_true = .pred_class) %>%
  group_by(date) %>%
  summarize(sens_forecast = sens_vec(class, pred_forecast),
            spec_forecast = spec_vec(class, pred_forecast),
            sens_true = sens_vec(class, pred_true),
            spec_true = spec_vec(class, pred_true)) %>%
  pivot_longer(sens_forecast:spec_true, names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```


```{r}
library(fable)
library(tsibble)
```

```{r cache = TRUE}
forecast <-
  processed_data %>%
  filter(date >= "2020-03-15" & date <= "2021-01-01") %>%
  as_tsibble(index = date, key = client) %>%
  model(arima = ARIMA(smoothed_deaths)) %>%
  forecast(h = "4 months")
```

We create a new set called "forecasted_test" out of our testing set where we replace "smoothed_deaths" by "forecasted_deaths".

```{r}
forecasted_test <-
  forecast %>%
  as_tibble() %>%
  select(client, date, .mean) %>%
  right_join(test, by = c("client", "date")) %>%
  select(-smoothed_deaths) %>%
  rename(smoothed_deaths = .mean)
```

Now, the awaited part! We are ready to introduce our modeling strategy! 

We first start by introducing a common recipe that we will use for all our models. Our target variable is class, we use all the predictors in weekly_data except for client, zip3, claims, smoothed_ae, shrunk_ae, ae, zip_deaths, ihme_deaths and date. We normalize all predictors and we apply log to both Volume of the client and Population of the zip code.  
```{r}
common_recipe <-
  recipe(class ~ ., data = weekly_data) %>%
  step_rm(client, zip3, claims, smoothed_ae, shrunk_ae,  ae, zip_deaths, ihme_deaths, date) %>%
  step_zv(all_predictors()) %>%
  step_log(volume, POP) %>%
  step_normalize(all_predictors())
```

Now, that we have our recipe, we are ready to try out different models and report the results. Let us introduce the six models and then we will talk a little bit about each one of them. 

```{r}
forest_spec <-
  rand_forest(trees = 1000) %>%
  set_engine("ranger", num.threads = 8, seed = 123456789) %>%
  set_mode("classification")

log_spec <- 
  logistic_reg(
  mode = "classification",
  engine = "glm")

svm_lin_spec <-
  svm_linear() %>%
  set_engine("LiblineaR") %>%
  set_mode("classification")

knn_spec <-
  nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

sln_spec <-
  mlp(activation = "relu", hidden_units = 6, epochs = 100) %>%
  set_engine("keras", verbose=0) %>%
  set_mode("classification")


bt_spec <- boost_tree(
  mode = "classification",
  engine = "xgboost",
  trees = 100)
```

We use Random Forest (forest), Logistic Regression (log), SVM (linear kernel), Nearest Neighbor (knn), Neural Network with single layer (sln) and Boosted Trees (bt) respectively with the default setting. For the Random Forest, we consider 1000 trees and for the Boosted Trees, we take 100 trees. All of these models use different engines introduced in tidymodels, and they are all set to mode=classficiation. 

We then create the workflow for the six models mentioned above with the recipe taken to be the "common recipe" and the model taken to be the ones introduced in the previous chunk. 

```{r}
bt_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(bt_spec)

log_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(log_spec)

forest_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(forest_spec)

svm_lin_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(svm_lin_spec)

knn_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(knn_spec)

sln_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(sln_spec)
```

We can take each of these models and evaluate their performance separately, but we want to find a way where we can compare their performance through time. So, we create a tribble containing the six different workflows, we fit out training set and we predict our forecasted_test. For the prediction, we use class_predict to come up with a class (this prediction will be used to calculate accuracy, sensitivity and specificity). We also use prob_predict (by adding type="prob") to come up with a predicitive probability (used to calculate the ROC_AUC). 

```{r cache = TRUE}
wflows <- tribble(~wflow ,
                  sln_wf,
                  knn_wf, log_wf, forest_wf, bt_wf)
 


wflows <-
  wflows %>%
  mutate(wflows_fit = map(wflow, ~ fit(.x, train))) 

wflows <-
  wflows %>%
  mutate(
    class_predict = map(wflows_fit, ~ predict(.x, forecasted_test)),  
    prob_predict = map(wflows_fit, ~ predict(.x, forecasted_test, type = "prob")))
```

Now that we have our prediction as a class and as a probability, we are ready to compare the metrics for the five models. 
```{r}
wflows %>%
  bind_cols(tribble(~id, "neural network", "nearest neigbor", "logistic regression", " Random Forest", "Boosted Trees")) %>%
  select(-wflow, -wflows_fit) %>%
  mutate(prob_predict = map(prob_predict, ~ bind_cols(.x, test %>% select(date, class)))) %>%
  unnest(c(class_predict, prob_predict)) %>%
  group_by(id, date) %>%
  summarize(
            sens = sens_vec(class, .pred_class),
            spec = spec_vec(class, .pred_class),
            roc_auc = roc_auc_vec(class, .pred_Adverse),
            accuracy = accuracy_vec(class, .pred_class), .groups = "keep") %>%
  pivot_longer(sens:accuracy, names_to = "metric", values_to = "value") %>%
  ungroup() %>%
  ggplot(aes(x = date, y = value, color = id)) +
  geom_point() +
  geom_line() +
  facet_wrap( ~ metric)
```

### Other models

Now, let's check other models and see if we can find better performance. First, we start with SVM_poly.

### Let's try to do some cross_validation before! 

```{r}
set.seed(100)

cv_folds <-
 vfold_cv(train, 
          v = 10, 
          strata = class) 

```

```{r}

log_spec <- 
  logistic_reg(
  mode = "classification",
  engine = "glm")

common_recipe <-
  recipe(class ~ ., data = processed_data) %>%
  step_rm(client, zip3, date, claims, smoothed_ae, shrunk_ae, shrinkage, dep_var, ae, smoothed_deaths) %>%
  step_zv(all_predictors())
```


```{r}
log_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(log_spec)

trained_log_wf <-
  log_wf %>%
  fit(train)
```

```{r}
forecasted_test <-
  forecast %>%
  as_tibble() %>%
  select(client, date, .mean) %>%
  right_join(test, by = c("client", "date")) %>%
  select(-zip_deaths) %>%
  rename(zip_deaths = .mean)

trained_log_wf %>%
  predict(forecasted_test, type = "prob") %>%
  bind_cols(test) %>%
  group_by(date) %>%
  summarize(roc_auc = roc_auc_vec(class, .pred_Adverse)) %>%
  ggplot(aes(x = date, y = roc_auc)) + geom_line()
```
How well can we possibly do? Let's use the true deaths, and compare with our forecasted ones.
Doesn't look too bad. Maybe a random forest is not the best model ?
```{r}
test %>%
  bind_cols(predict(trained_log_wf, forecasted_test)) %>%
  rename(pred_forecast = .pred_class) %>%
  bind_cols(predict(trained_log_wf, test)) %>%
  rename(pred_true = .pred_class) %>%
  group_by(date) %>%
  summarize(sens_forecast = sens_vec(class, pred_forecast),
            spec_forecast = spec_vec(class, pred_forecast),
            sens_true = sens_vec(class, pred_true),
            spec_true = spec_vec(class, pred_true)) %>%
  pivot_longer(sens_forecast:spec_true, names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```

Logistic regression is not good! Let's check SVM! 

```{r}



svm_lin_spec <-
  svm_linear() %>%
  set_engine("LiblineaR") %>%
  set_mode("classification")

common_recipe <-
  recipe(class ~ ., data = processed_data) %>%
  step_rm(client, zip3, claims, date, smoothed_ae, shrunk_ae, shrinkage, dep_var, ae, smoothed_deaths) %>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())

```


```{r}
svm_lin_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(svm_lin_spec)

trained_svm_lin_wf <-
  svm_lin_wf %>%
  fit(train)
```

```{r}
forecasted_test <-
  forecast %>%
  as_tibble() %>%
  select(client, date, .mean) %>%
  right_join(test, by = c("client", "date")) %>%
  select(-zip_deaths) %>%
  rename(zip_deaths = .mean)

#trained_svm_lin_wf %>%
 # predict(forecasted_test, type = "prob") %>%
  #bind_cols(test) %>%
  #group_by(date) %>%
  #summarize(roc_auc = roc_auc_vec(class, .pred_Adverse)) %>%
  #ggplot(aes(x = date, y = roc_auc)) + geom_line()
```

How well can we possibly do? Let's use the true deaths, and compare with our forecasted ones.
Doesn't look too bad. Maybe a random forest is not the best model ?
```{r}
test %>%
  bind_cols(predict(trained_svm_lin_wf, forecasted_test)) %>%
  rename(pred_forecast = .pred_class) %>%
  bind_cols(predict(trained_svm_lin_wf, test)) %>%
  rename(pred_true = .pred_class) %>%
  group_by(date) %>%
  summarize(sens_forecast = sens_vec(class, pred_forecast),
            spec_forecast = spec_vec(class, pred_forecast),
            sens_true = sens_vec(class, pred_true),
            spec_true = spec_vec(class, pred_true)) %>%
  pivot_longer(sens_forecast:spec_true, names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```

Something is wrong with SVM poly. Let's try KNN. 

```{r}

knn_spec <-
  nearest_neighbor() %>%
  set_engine("kknn") %>%
  set_mode("classification")

common_recipe <-
  recipe(class ~ ., data = processed_data) %>%
  step_rm(client, zip3, claims, smoothed_ae, shrunk_ae, shrinkage, dep_var, ae, smoothed_deaths) %>%
  step_zv(all_predictors())
```


```{r}
knn_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(knn_spec)

trained_knn_wf <-
  knn_wf %>%
  fit(train)
```

```{r}
forecasted_test <-
  forecast %>%
  as_tibble() %>%
  select(client, date, .mean) %>%
  right_join(test, by = c("client", "date")) %>%
  select(-zip_deaths) %>%
  rename(zip_deaths = .mean)

trained_knn_wf %>%
  predict(forecasted_test, type = "prob") %>%
  bind_cols(test) %>%
  group_by(date) %>%
  summarize(roc_auc = roc_auc_vec(class, .pred_Adverse)) %>%
  ggplot(aes(x = date, y = roc_auc)) + geom_line()
```

How well can we possibly do? Let's use the true deaths, and compare with our forecasted ones.
Doesn't look too bad. Maybe a random forest is not the best model ?
```{r}
test %>%
  bind_cols(predict(trained_knn_wf, forecasted_test)) %>%
  rename(pred_forecast = .pred_class) %>%
  bind_cols(predict(trained_knn_wf, test)) %>%
  rename(pred_true = .pred_class) %>%
  group_by(date) %>%
  summarize(sens_forecast = sens_vec(class, pred_forecast),
            spec_forecast = spec_vec(class, pred_forecast),
            sens_true = sens_vec(class, pred_true),
            spec_true = spec_vec(class, pred_true)) %>%
  pivot_longer(sens_forecast:spec_true, names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```
It looks like KNN is good. 


Let's check Neural Network now! 

```{r}

sln_spec <-
  mlp(activation = "relu", hidden_units = 6, epochs = 100) %>%
  set_engine("keras", verbose=0) %>%
  set_mode("classification")

common_recipe <-
  recipe(class ~ ., data = processed_data) %>%
  step_rm(client, zip3, claims, smoothed_ae, shrunk_ae, shrinkage, dep_var, ae, smoothed_deaths, date) %>%
  step_zv(all_predictors())%>%
  step_normalize(all_predictors())
```


```{r}
sln_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(sln_spec)

trained_sln_wf <-
  sln_wf %>%
  fit(train)
```

```{r}
forecasted_test <-
  forecast %>%
  as_tibble() %>%
  select(client, date, .mean) %>%
  right_join(test, by = c("client", "date")) %>%
  select(-zip_deaths) %>%
  rename(zip_deaths = .mean)

trained_sln_wf %>%
  predict(forecasted_test, type = "prob") %>%
  bind_cols(test) %>%
  group_by(date) %>%
  summarize(roc_auc = roc_auc_vec(class, .pred_Adverse)) %>%
  ggplot(aes(x = date, y = roc_auc)) + geom_line()
```

How well can we possibly do? Let's use the true deaths, and compare with our forecasted ones.
Doesn't look too bad. Maybe a random forest is not the best model ?

```{r}
test %>%
  bind_cols(predict(trained_sln_wf, forecasted_test)) %>%
  rename(pred_forecast = .pred_class) %>%
  bind_cols(predict(trained_sln_wf, test)) %>%
  rename(pred_true = .pred_class) %>%
  group_by(date) %>%
  summarize(sens_forecast = sens_vec(class, pred_forecast),
            spec_forecast = spec_vec(class, pred_forecast),
            sens_true = sens_vec(class, pred_true),
            spec_true = spec_vec(class, pred_true)) %>%
  pivot_longer(sens_forecast:spec_true, names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```



Neural Network is horrible. Let's move to boosted trees. 

```{r}

bt_spec <- boost_tree(
  mode = "classification",
  engine = "xgboost",
  trees = 100)

common_recipe <-
  recipe(class ~ ., data = processed_data) %>%
  step_rm(client, zip3, date, claims, smoothed_ae, shrunk_ae, shrinkage, dep_var, ae, smoothed_deaths) %>%
  step_zv(all_predictors())
```


```{r}
bt_wf <-
  workflow() %>%
  add_recipe(common_recipe) %>%
  add_model(bt_spec)

trained_bt_wf <-
  bt_wf %>%
  fit(train)
```

```{r}
forecasted_test <-
  forecast %>%
  as_tibble() %>%
  select(client, date, .mean) %>%
  right_join(test, by = c("client", "date")) %>%
  select(-zip_deaths) %>%
  rename(zip_deaths = .mean)

trained_bt_wf %>%
  predict(forecasted_test, type = "prob") %>%
  bind_cols(test) %>%
  group_by(date) %>%
  summarize(roc_auc = roc_auc_vec(class, .pred_Adverse)) %>%
  ggplot(aes(x = date, y = roc_auc)) + geom_line()
```

How well can we possibly do? Let's use the true deaths, and compare with our forecasted ones.
Doesn't look too bad. Maybe a random forest is not the best model ?
```{r}
test %>%
  bind_cols(predict(trained_knn_wf, forecasted_test)) %>%
  rename(pred_forecast = .pred_class) %>%
  bind_cols(predict(trained_knn_wf, test)) %>%
  rename(pred_true = .pred_class) %>%
  group_by(date) %>%
  summarize(sens_forecast = sens_vec(class, pred_forecast),
            spec_forecast = spec_vec(class, pred_forecast),
            sens_true = sens_vec(class, pred_true),
            spec_true = spec_vec(class, pred_true)) %>%
  pivot_longer(sens_forecast:spec_true, names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```

Boosted Trees is great! 


### Change the death_zip variable

We will use a smoothed `zip_death` variable instead!
```{r}
forest_recipe <-
  recipe(class ~ ., data = processed_data) %>%
  step_rm(client, zip3, claims, smoothed_ae, shrunk_ae, shrinkage, dep_var, ae, zip_deaths) %>%
  step_zv(all_predictors())

forest_wf <-
  workflow() %>%
  add_recipe(forest_recipe) %>%
  add_model(forest_spec)

trained_wf <-
  forest_wf %>%
  fit(train)
```





We substitute `smoothed_deaths` by the forecasted version of `smoothed_deaths` and predict.
We plot the `roc_auc` of our predictions.
```{r}
forecasted_test <-
  forecast %>%
  as_tibble() %>%
  select(client, date, .mean) %>%
  right_join(test, by = c("client", "date")) %>%
  select(-smoothed_deaths) %>%
  rename(smoothed_deaths = .mean)

trained_wf %>%
  predict(forecasted_test, type = "prob") %>%
  bind_cols(test) %>%
  group_by(date) %>%
  summarize(roc_auc = roc_auc_vec(class, .pred_Adverse)) %>%
  ggplot(aes(x = date, y = roc_auc)) + geom_line()
```

Again, we compare with the true value of `smoothed_deaths`. It's much closer!!!!
```{r}
test %>%
  bind_cols(predict(trained_wf, forecasted_test)) %>%
  rename(pred_forecast = .pred_class) %>%
  bind_cols(predict(trained_wf, test)) %>%
  rename(pred_true = .pred_class) %>%
  group_by(date) %>%
  summarize(sens_forecast = sens_vec(class, pred_forecast),
            spec_forecast = spec_vec(class, pred_forecast),
            sens_true = sens_vec(class, pred_true),
            spec_true = spec_vec(class, pred_true)) %>%
  pivot_longer(sens_forecast:spec_true, names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```

Boosted Trees! 
```{r}

bt_spec <- boost_tree(
  mode = "classification",
  engine = "xgboost",
  trees = 100)

bt_recipe <-
  recipe(class ~ ., data = processed_data) %>%
  step_rm(client, zip3, date, claims, smoothed_ae, shrunk_ae, shrinkage, dep_var, ae, zip_deaths) %>%
  step_zv(all_predictors())
```


```{r}
bt2_wf <-
  workflow() %>%
  add_recipe(bt_recipe) %>%
  add_model(bt_spec)

trained_bt2_wf <-
  bt2_wf %>%
  fit(train)
```
We substitute `smoothed_deaths` by the forecasted version of `smoothed_deaths` and predict.
We plot the `roc_auc` of our predictions.
```{r}
forecasted_test <-
  forecast %>%
  as_tibble() %>%
  select(client, date, .mean) %>%
  right_join(test, by = c("client", "date")) %>%
  select(-smoothed_deaths) %>%
  rename(smoothed_deaths = .mean)

trained_bt2_wf %>%
  predict(forecasted_test, type = "prob") %>%
  bind_cols(test) %>%
  group_by(date) %>%
  summarize(roc_auc = roc_auc_vec(class, .pred_Adverse)) %>%
  ggplot(aes(x = date, y = roc_auc)) + geom_line()
```

Again, we compare with the true value of `smoothed_deaths`. It's much closer!!!!
```{r}
test %>%
  bind_cols(predict(trained_bt2_wf, forecasted_test)) %>%
  rename(pred_forecast = .pred_class) %>%
  bind_cols(predict(trained_bt2_wf, test)) %>%
  rename(pred_true = .pred_class) %>%
  group_by(date) %>%
  summarize(sens_forecast = sens_vec(class, pred_forecast),
            spec_forecast = spec_vec(class, pred_forecast),
            sens_true = sens_vec(class, pred_true),
            spec_true = spec_vec(class, pred_true)) %>%
  pivot_longer(sens_forecast:spec_true, names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```



### Using IHME forecasts

We will use a more trustworthy forecast than our naive ARIMA...
This will require some data wrangling ðŸ¥´ 

We start by loading state name and FIPS codes
```{r}
states <-
  read_delim("data/state.txt", delim = "|") %>%
  select(STATE, STATE_NAME)
```


We augment the main dataset by adding the state name
```{r}
zip_to_state <-
  read_csv("data/zcta_county_rel_10.txt") %>%
  select(ZCTA5, STATE) %>%
  mutate(zip3 = str_sub(ZCTA5, 1, 3), .keep = "unused") %>%
  group_by(zip3) %>%
  count(STATE, sort = TRUE) %>%
  slice_head() %>%
  ungroup() %>%
  left_join(states, by = "STATE") %>%
  select(-STATE, -n)

processed_data <-
  processed_data %>%
  nest_by(zip3) %>%
  left_join(zip_to_state, by = "zip3") %>%
  unnest(cols = c(data))
```






Next we read the data from IHME as of Dec 23 2020
```{r}
ihme <-
  read_csv("data/2020_12_23/reference_hospitalization_all_locs.csv") %>%
  rename(STATE_NAME = location_name) %>%
  semi_join(processed_data, by = "STATE_NAME") %>%
  select(STATE_NAME, date, deaths_mean) %>%
  mutate(date = ceiling_date(date, unit = "week")) %>%
  group_by(STATE_NAME, date) %>%
  summarize(ihme_deaths = sum(deaths_mean))
```

Add to `processed_data`
```{r}
processed_data <-
  processed_data %>%
  left_join(ihme, by = c("date", "STATE_NAME")) %>%
  ungroup() %>%
  mutate(ihme_deaths = replace_na(ihme_deaths, 0))
```

Now we can go back to our usual training testing thing. Note that for IHME, the last forecasted date is `2021-04-04`.
```{r}
train <-
  processed_data %>%
  filter(date <= "2021-01-01")

test <-
  processed_data %>%
  filter(date > "2021-01-01" & date <= "2021-04-04")
```

Now the usual workflow stuff
```{r}
forest_recipe <-
  recipe(class ~ ., data = processed_data) %>%
  step_rm(zip3, client, claims, zip_deaths, smoothed_ae, shrunk_ae, smoothed_deaths, ae, dep_var, shrinkage, STATE_NAME) %>%
  step_zv(all_predictors())

forest_wf <-
  workflow() %>%
  add_recipe(forest_recipe) %>%
  add_model(forest_spec)

trained_wf <-
  forest_wf %>%
  fit(train)
```

We plot the `roc_auc` of our predictions.
```{r}
trained_wf %>%
  predict(test, type = "prob") %>%
  bind_cols(test) %>%
  group_by(date) %>%
  summarize(roc_auc = roc_auc_vec(class, .pred_Adverse)) %>%
  ggplot(aes(x = date, y = roc_auc)) + geom_line()
```

We can look at sensitivity and specificity
```{r}
test %>%
  bind_cols(predict(trained_wf, test)) %>%
  group_by(date) %>%
  summarize(sens_forecast = sens_vec(class, .pred_class),
            spec_forecast = spec_vec(class, .pred_class)) %>%
  pivot_longer(sens_forecast:spec_forecast, names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```

### smoothed_deaths with XGBoost

We will try to predict the class three months in the future using a forecasted `smoothed_deaths` and `ihme_deaths`.

We will use an XGBoost model.
```{r}
xgboost_recipe <-
  recipe(formula = class ~ ., data = processed_data) %>%
  step_rm(zip3, client, claims, zip_deaths, smoothed_ae, shrunk_ae, ae, dep_var, shrinkage) %>%
  step_rm(STATE_NAME, date) %>%
  step_zv(all_predictors())

xgboost_spec <-
  # boost_tree(trees = tune(), min_n = tune(), tree_depth = tune(), learn_rate = tune(), loss_reduction = tune(), sample_size = tune()) %>%
  boost_tree() %>%
  set_mode("classification") %>%
  set_engine("xgboost")

xgboost_workflow <-
  workflow() %>%
  add_recipe(xgboost_recipe) %>%
  add_model(xgboost_spec)
```


First we forecast `smoothed_deaths`.
```{r}
start <- ceiling_date(ymd("2021-01-01"), unit = "week")
end <- ceiling_date(ymd("2020-04-04"), unit = "week")

forecast <-
  processed_data %>%
  filter(date >= "2020-03-15" & date <= start) %>%
  as_tsibble(index = date, key = client) %>%
  model(arima = ARIMA(smoothed_deaths)) %>%
  forecast(h = "4 months")
```

We plot it to see if it makes sense
```{r}
forecast %>%
  filter(client == 30) %>%
  autoplot(processed_data %>% as_tsibble(index = date, key = client) %>% filter(client == 30))
```

Substitute actual deaths after `start` by the forecasted ones
```{r}
future <-
  forecast %>%
  as_tibble() %>%
  select(date, client, .mean) %>%
  # inner_join(processed_data, by = c("date", "client")) %>%
  # select(-smoothed_deaths) %>%
  rename(smoothed_deaths = .mean)

forecasted_data <-
  processed_data %>%
  rows_update(future, by = c("date", "client"))
```

Create the `rsample` object. We will test on one week, namely the week of Apr 4th.
```{r}
train_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  filter(date <= start) %>%
  pull(rowname) %>%
  as.integer()

test_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  filter(date == ceiling_date(end, unit = "week")) %>%
  pull(rowname) %>%
  as.integer()

split <- make_splits(list(analysis = train_idx, assessment = test_idx), forecasted_data)
rset <- manual_rset(list(split), ids = c("Split 1"))
```

Fit and test
```{r}
results <-
  xgboost_workflow %>%
  fit_resamples(
      resamples = rset,
      control = control_resamples(save_pred = TRUE),
      metrics = metric_set(yardstick::accuracy, roc_auc, sens, spec, j_index))


results %>%
  collect_metrics()

xgboost_workflow %>%
  fit(analysis(split))
```

### Tuning a boosted tree
We will first split the clients into testing and training
```{r}
train_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  nest_by(client) %>%
  ungroup() %>%
  slice_sample(prop = 3/4) %>%
  unnest(data) %>%
  pull(rowname) %>%
  as.integer()

test_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  select(rowname) %>%
  filter(!rowname %in% train_idx) %>%
  pull(rowname) %>%
  as.integer()

init <- make_splits(list(analysis = train_idx, assessment = test_idx), data = forecasted_data)
```

Next, we pick the last 3 months of 2020 as a testing set and everything before that as training.
We use 10-fold CV on each client. We will use the 3 months before 2021-01-01 as training, and 2021-01-01 as testing.
```{r}
make_cv <-
  training(init) %>%
  rownames_to_column() %>%
  group_by(client) %>%
  nest() %>%
  vfold_cv(v = 5) %>%
  mutate(
    an = map(splits, ~ analysis(.) %>% unnest(data) %>% ungroup()),
    as = map(splits, ~ assessment(.) %>% unnest(data) %>% ungroup())) %>%
  mutate(
    an_idx = map(an, ~ filter(., date <= start - months(3)) %>% pull(rowname) %>% as.integer()),
    as_idx = map(as, ~ filter(., date == start) %>% pull(rowname) %>% as.integer())) %>%
  mutate(splits = map2(an_idx, as_idx, ~ make_splits(list(analysis = .x, assessment = .y), data = training(init)))) %>%
  select(splits, id)

resamp <- manual_rset(make_cv$splits, make_cv$id)
```

Define an XGBoost model
```{r}
xgboost_recipe <-
  recipe(formula = class ~ ., data = processed_data) %>%
  step_rm(zip3, client, claims, zip_deaths, smoothed_ae, shrunk_ae, ae, dep_var, shrinkage) %>%
  step_rm(STATE_NAME, date) %>%
  step_zv(all_predictors())

xgboost_spec <-
  boost_tree(trees = tune(), tree_depth = tune(), learn_rate = tune()) %>%
  set_mode("classification") %>%
  set_engine("xgboost", nthread = 8)

xgboost_workflow <-
  workflow() %>%
  add_recipe(xgboost_recipe) %>%
  add_model(xgboost_spec)
```

We tune. This might take a while so I chose a nice tuning value at the end.
```{r cache = TRUE}
res <-
  xgboost_workflow %>%
  tune_grid(
      resamples = resamp,
      grid = 10,
      control = control_grid(save_pred = FALSE, verbose = TRUE, parallel_over = "everything"),
      metrics = metric_set(yardstick::accuracy, roc_auc, sens, spec, j_index))

res %>% show_best(metric = "roc_auc")
.Last.value
autoplot(res)

parms <-
  xgboost_workflow %>%
  parameters()

res <-
  xgboost_workflow %>%
  tune_sim_anneal(
      resamples = resamp,
      param_info = parms,
      iter = 20,
      initial = res,
      metrics = metric_set(roc_auc, sens, spec, j_index, yardstick::accuracy))


res %>% show_best(metric = "roc_auc")
.Last.value
autoplot(res)

bst <- res %>% select_best(metric = "roc_auc")

bst <-
 tribble(~trees, ~tree_depth, ~learn_rate,
         138, 6, 0.07662993111378884)

xgboost_workflow <- finalize_workflow(xgboost_workflow, bst)
```

Now we train on all clients prior to 2021-01-01 and test on 20201-04-04
```{r}
train_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  filter(date <= start) %>%
  pull(rowname) %>% as.integer()

test_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  filter(date == end) %>%
  pull(rowname) %>% as.integer()

s1 <- make_splits(list(analysis = train_idx, assessment = test_idx), data = forecasted_data)
rs <- manual_rset(list(s1), c("all_clients"))

xgboost_workflow %>%
  last_fit(s1, metrics = metric_set(roc_auc, sens, spec, j_index, yardstick::accuracy)) %>%
  collect_metrics()
```

Next we train on **some** clients, and test on seen and unseen clients!!!
```{r}
train_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  nest_by(client) %>%
  ungroup() %>%
  slice_sample(prop = 3/4) %>%
  unnest(data) %>%
  filter(date <= start) %>%
  pull(rowname) %>%
  as.integer()

train_clients <-
  forecasted_data %>%
  dplyr::slice(train_idx) %>%
  pull(client) %>%
  unique()

unk_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  filter(!client %in% train_clients) %>%
  filter(date == end) %>%
  pull(rowname) %>%
  as.integer()

knw_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  filter(client %in% train_clients) %>%
  filter(date == end) %>%
  pull(rowname) %>%
  as.integer()

kno_for <- make_splits(list(analysis = train_idx, assessment = knw_idx), data = forecasted_data)
unk_for <- make_splits(list(analysis = train_idx, assessment = unk_idx), data = forecasted_data)
kno_tru <- make_splits(list(analysis = train_idx, assessment = knw_idx), data = processed_data)
unk_tru <- make_splits(list(analysis = train_idx, assessment = unk_idx), data = processed_data)
rset <- manual_rset(list(kno_for, unk_for, kno_tru, unk_tru), ids = c("Known Forecast", "Unknown Forecast", "Known True", "Unknown True"))

res <-
  xgboost_workflow %>%
  fit_resamples(
        resamples = rset,
        metrics = metric_set(roc_auc, yardstick::accuracy, sens, spec, j_index))

res %>%
  collect_metrics(summarize = FALSE) %>%
  mutate(known = str_detect(id, "Known"), fore = str_detect(id, "Forecast")) %>%
  ggplot(aes(x = as.factor(id), y = .estimate, color = .metric)) +
  geom_point() +
  geom_line(aes(group = interaction(.metric, known)))
  # geom_line(aes(group = interaction(.metric, fore)), color = "grey")

forecasted_data %>%
  filter(date == end) %>%
  count(class)
```

### The above with MARS
We choose the tuning data. This is a subset of clients
```{r}
train_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  nest_by(client) %>%
  ungroup() %>%
  slice_sample(prop = 3/4) %>%
  unnest(data) %>%
  pull(rowname) %>%
  as.integer()

test_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  select(rowname) %>%
  filter(!rowname %in% train_idx) %>%
  pull(rowname) %>%
  as.integer()
```

Next, we pick the last 3 months of 2020 as a testing set and everything before that as training.
We use 10-fold CV on each client. We will use the 3 months before 2021-01-01 as training, and 2021-01-01 as testing.
```{r}
make_cv <-
  training(init) %>%
  rownames_to_column() %>%
  group_by(client) %>%
  nest() %>%
  vfold_cv(v = 5) %>%
  mutate(
    an = map(splits, ~ analysis(.) %>% unnest(data) %>% ungroup()),
    as = map(splits, ~ assessment(.) %>% unnest(data) %>% ungroup())) %>%
  mutate(
    an_idx = map(an, ~ filter(., date <= start - months(3)) %>% pull(rowname) %>% as.integer()),
    as_idx = map(as, ~ filter(., date == start) %>% pull(rowname) %>% as.integer())) %>%
  mutate(splits = map2(an_idx, as_idx, ~ make_splits(list(analysis = .x, assessment = .y), data = training(init)))) %>%
  select(splits, id)

resamp <- manual_rset(make_cv$splits, make_cv$id)
```

Define an MARS model
```{r}
earth_recipe <-
  recipe(formula = class ~ ., data = processed_data) %>%
  step_rm(zip3, client, claims, zip_deaths, smoothed_ae, shrunk_ae, ae, dep_var, shrinkage) %>%
  step_rm(STATE_NAME, date) %>%
  step_zv(all_predictors())

earth_spec <-
  mars(num_terms = tune(), prod_degree = tune(), prune_method = "backward") %>%
  set_mode("classification") %>%
  set_engine("earth")

earth_workflow <-
  workflow() %>%
  add_recipe(earth_recipe) %>%
  add_model(earth_spec)

earth_grid <-
  earth_workflow %>%
  parameters() %>%
  update(
    prune_method = prune_method(c("forward","backward","none")),
    num_terms = num_terms(c(1, 30))) %>%
  grid_regular(levels = 5)
```

We look at results
```{r cache = TRUE}
res <-
  earth_workflow %>%
  tune_grid(
      resamples = resamp,
      grid = earth_grid,
      control = control_grid(save_pred = FALSE, verbose = TRUE, parallel_over = "everything"),
      metrics = metric_set(yardstick::accuracy, roc_auc, sens, spec, j_index))

res %>% show_best(metric = "roc_auc")
.Last.value
autoplot(res)

bst <- res %>% select_best(metric = "roc_auc")

earth_workflow <- finalize_workflow(earth_workflow, bst)
```

Now we train on all clients prior to 2021-01-01 and test on 20201-04-04
```{r}
train_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  filter(date <= start) %>%
  pull(rowname) %>% as.integer()

test_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  filter(date == end) %>%
  pull(rowname) %>% as.integer()

s1 <- make_splits(list(analysis = train_idx, assessment = test_idx), data = forecasted_data)
rs <- manual_rset(list(s1), c("all_clients"))

earth_workflow %>%
  last_fit(s1, metrics = metric_set(roc_auc, sens, spec, j_index, yardstick::accuracy)) %>%
  collect_metrics()
```

Next we train on **some** clients, and test on seen and unseen clients!!!
```{r}
train_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  nest_by(client) %>%
  ungroup() %>%
  slice_sample(prop = 3/4) %>%
  unnest(data) %>%
  filter(date <= start) %>%
  pull(rowname) %>%
  as.integer()

train_clients <-
  forecasted_data %>%
  dplyr::slice(train_idx) %>%
  pull(client) %>%
  unique()

unk_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  filter(!client %in% train_clients) %>%
  filter(date == end) %>%
  pull(rowname) %>%
  as.integer()

knw_idx <-
  forecasted_data %>%
  rownames_to_column() %>%
  filter(client %in% train_clients) %>%
  filter(date == end) %>%
  pull(rowname) %>%
  as.integer()

s1 <- make_splits(list(analysis = train_idx, assessment = knw_idx), data = forecasted_data)
s2 <- make_splits(list(analysis = train_idx, assessment = unk_idx), data = forecasted_data)
rset <- manual_rset(list(s1, s2), ids = c("Known", "Unknown"))

res <-
  xgboost_workflow %>%
  fit_resamples(
        resamples = rset,
        metrics = metric_set(roc_auc, yardstick::accuracy, sens, spec, j_index))

res %>%
  collect_metrics(summarize = FALSE) %>%
  ggplot(aes(x = as.factor(id), y = .estimate, color = .metric, group = .metric)) + geom_point() + geom_line()

forecasted_data %>%
  filter(date == end) %>%
  count(class)
```


# Forecasting AE directly
Get weeklydata from time.Rmd
```{r}
clients <-
  read_feather("data/process.feather") %>%
  filter(date >= "2020-03-15") %>%
  mutate(client = as.factor(client)) %>%
  #select(-year) %>%
  mutate(POP=log(POP)) %>%
  mutate(volume = log(volume)) %>%
  mutate(expected = log(expected))
```

split
```{r}
#split
  split
  set.seed = 1234
  splits <-  clients %>%
    time_series_split(initial = "6 months", assess = "6 months", date_var = date, cumulative = TRUE)
  train = training(splits)
  test = testing(splits)
  #index <- unique(train$client)
  #val <- sliding_index(training(splits), index)
  
  #crossval <- vfold_cv(training(splits), strata = zip3)
```
#window forest
It is not good 
```{r}
model_spec <- window_reg(
        window_size     = "6 months",
        id = "client"
    ) %>%
    # Extra parameters passed as: set_engine(...)
    set_engine(
        engine          = "window_function",
        #window_function = median,
        na.rm           = TRUE,
        window_function = ~ tail(.x, 100),
    )

# Fit Spec
model_fit <- model_spec %>%
    fit(shrunk_ae~ date + client, data = train)
model_fit
predict(model_fit, test)


model_tbl <- modeltime_table(
    model_fit)

model_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = bind_rows(training(splits), testing(splits)),
        conf_by_id  = TRUE
    ) %>%
    group_by(client) %>%
    filter(client == c( "1", "5", "7"))%>%
    plot_modeltime_forecast(
        .facet_ncol  = 3,
        .interactive = FALSE,
        .title = "Forecast Plot",
        .line_alpha = 0.6,
        .line_size = 1,
        .y_intercept = 3.0
    )
calib_tbl <- model_tbl %>%
    modeltime_calibrate(
      new_data = testing(splits), 
      id       = "client"
    )
calib_tbl %>% 
    modeltime_accuracy(acc_by_id = FALSE) %>% 
    table_modeltime_accuracy(.interactive = FALSE)
calib_tbl %>% 
    modeltime_accuracy(acc_by_id = TRUE) %>% 
    table_modeltime_accuracy(.interactive = FALSE)

```

```{r}
result0 <- calib_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = bind_rows(training(splits), testing(splits)),
        conf_by_id  = TRUE
    ) 
result0
```

#Compare only one predictor(date) with all predictors(as before), the latter has better results.
#XGboost have the best result among all 7 model.

We now gather our recipes and models.

#recipe
```{r}
rec_obj <-
    recipe(shrunk_ae ~ ., data = training(splits)) %>%
    #step_rm(year,month,day)%>%
    step_rm(zip3)%>%
    step_rm( claims, smoothed_ae, class, shrinkage, dep_var, ae, zip_deaths)%>%
    #step_rm(adverse)%>%
    step_mutate(client = droplevels(client)) %>%
    step_timeseries_signature(date) %>%
    step_rm(date)%>%
    step_dummy(all_nominal_predictors(), one_hot = TRUE)%>%
     step_zv(all_predictors()) %>%
    step_normalize(all_predictors(), -all_nominal())

summary(prep(rec_obj))
bake(prep(rec_obj),training(splits))
```

#engine
```{r}
forest_spec <-
  rand_forest(trees = 1000) %>%
  set_engine("ranger", num.threads = 8, seed = 123456789) %>%
  #set_mode("classification") %>%
  set_mode("regression")%>%
  set_engine("ranger", num.threads = 8, importance = "impurity", seed = 123)
tuned_forest_spec <-
  rand_forest(trees = 1000, mtry = 12, min_n = 21) %>%
  #set_mode("classification") %>%
  set_mode("regression")%>%
  set_engine("ranger", num.threads = 8, importance = "impurity", seed = 123)
# Samara's models
sln_spec <-
  mlp(activation = "relu", hidden_units = 6, epochs = 100) %>%
  set_engine("nnet") %>%
  #set_mode("classification")
  set_mode("regression")
svm_rbf_spec <-
  svm_rbf() %>%
  set_engine("kernlab") %>%
  #set_mode("classification")
  set_mode("regression")
svm_poly_spec <-
  svm_poly() %>%
  set_engine("kernlab") %>%
  #set_mode("classification")
  set_mode("regression")
knn_spec <-
  nearest_neighbor() %>%
  set_engine("kknn") %>%
  #set_mode("classification")
  set_mode("regression")
xgboost_spec <-
  boost_tree(trees = 100) %>%
  set_engine("xgboost") %>%
  #set_mode("classification")
  set_mode("regression")
```

# workflow without death data
```{r eval= FALSE}
#cannot fit
wflw_neural <- workflow() %>%
    add_model(
        sln_spec
    ) %>%
    add_recipe(rec_obj) %>%
    fit(data = training(splits))
```
```{r eval= FALSE}
#take a lot time()
wflw_svmpoly <- workflow() %>%
    add_model(
        svm_poly_spec
    ) %>%
    add_recipe(rec_obj) %>%
    fit(data = training(splits))
```

```{r}
wflw_rf <- workflow() %>%
    add_model(
        forest_spec
    ) %>%
    add_recipe(rec_obj) %>%
    fit(data = training(splits))

wflw_tunedrf <- workflow() %>%
    add_model(
        tuned_forest_spec
    ) %>%
    add_recipe(rec_obj) %>%
    fit(data = training(splits))

wflw_svmrbf <- workflow() %>%
    add_model(
        svm_rbf_spec
    ) %>%
    add_recipe(rec_obj) %>%
    fit(data = training(splits))

wflw_knnspec <- workflow() %>%
    add_model(
        knn_spec
    ) %>%
    add_recipe(rec_obj) %>%
    fit(data = training(splits))  

wflw_xgboost <- workflow() %>%
    add_model(
        xgboost_spec
    ) %>%
    add_recipe(rec_obj) %>%
    fit(data = training(splits))  
```
Create a Modeltime Table
```{r}
model_tbl <- modeltime_table(
    wflw_rf,
    wflw_tunedrf,
    #wflw_neural,
    wflw_svmrbf,
    #wflw_svmpoly,
    wflw_knnspec,
    wflw_xgboost
)
model_tbl
```

#Calibrate by client
```{r}
calib_tbl <- model_tbl %>%
    modeltime_calibrate(
      new_data = testing(splits), 
      id       = "client"
    )

calib_tbl
```

Measure Accuracy on validation data
```{r}
#global error
calib_tbl %>% 
    modeltime_accuracy(acc_by_id = FALSE) %>% 
    table_modeltime_accuracy(.interactive = FALSE)
```
```{r}
#local error for each client
calib_tbl %>% 
    modeltime_accuracy(acc_by_id = TRUE) %>% 
    table_modeltime_accuracy(.interactive = FALSE)
```
validate the  Test Data
conf_by_id : produce confidence interval estimates by an ID feature.
#all results
```{r}
result <- calib_tbl %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = bind_rows(training(splits), testing(splits)),
        conf_by_id  = TRUE
    ) 
result
```
```{r}
result %>%
    group_by(client) %>%
    filter(client == c( "1", "5", "7","10", "61","100"))%>%
    plot_modeltime_forecast(
        .facet_ncol  = 3,
        .interactive = FALSE,
        .title = "Forecast Plot",
        .line_alpha = 0.6,
        .line_size = 1,
        .y_intercept = 3.0
    )
```

#recipe with zip death
```{r}
rec_obj1 <-
    recipe(shrunk_ae ~ ., data = training(splits)) %>%
    #step_rm(year,month,day)%>%
    step_rm(zip3)%>%
    step_rm( claims, smoothed_ae, class, shrinkage, dep_var, ae)%>%
    #step_rm(adverse)%>%
    step_mutate(client = droplevels(client)) %>%
    step_timeseries_signature(date) %>%
    step_rm(date)%>%
    step_dummy(all_nominal_predictors(), one_hot = TRUE)%>%
     step_zv(all_predictors()) %>%
    step_normalize(all_predictors(), -all_nominal())

summary(prep(rec_obj1))
bake(prep(rec_obj1),training(splits))
```


# workflow with zip death

```{r}
wflw_rf1 <- workflow() %>%
    add_model(
        forest_spec
    ) %>%
    add_recipe(rec_obj1) %>%
    fit(data = training(splits))

wflw_tunedrf1 <- workflow() %>%
    add_model(
        tuned_forest_spec
    ) %>%
    add_recipe(rec_obj1) %>%
    fit(data = training(splits))

wflw_svmrbf1 <- workflow() %>%
    add_model(
        svm_rbf_spec
    ) %>%
    add_recipe(rec_obj1) %>%
    fit(data = training(splits))

wflw_knnspec1 <- workflow() %>%
    add_model(
        knn_spec
    ) %>%
    add_recipe(rec_obj1) %>%
    fit(data = training(splits))  

wflw_xgboost1 <- workflow() %>%
    add_model(
        xgboost_spec
    ) %>%
    add_recipe(rec_obj1) %>%
    fit(data = training(splits))  
```
Create a Modeltime Table
```{r}
model_tbl1 <- modeltime_table(
    wflw_rf1,
    wflw_tunedrf1,
    #wflw_neural,
    wflw_svmrbf1,
    #wflw_svmpoly,
    wflw_knnspec1,
    wflw_xgboost1
)
model_tbl1
```

#Calibrate by client
```{r}
calib_tbl1 <- model_tbl1 %>%
    modeltime_calibrate(
      new_data = testing(splits), 
      id       = "client"
    )

calib_tbl1
```

Measure Accuracy on validation data
```{r}
#global error
calib_tbl1 %>% 
    modeltime_accuracy(acc_by_id = FALSE) %>% 
    table_modeltime_accuracy(.interactive = FALSE)
```
```{r}
#local error for each client
calib_tbl1 %>% 
    modeltime_accuracy(acc_by_id = TRUE) %>% 
    table_modeltime_accuracy(.interactive = FALSE)
```
validate the  Test Data
conf_by_id : produce confidence interval estimates by an ID feature.
#all results
```{r}
result1 <- calib_tbl1 %>%
    modeltime_forecast(
        new_data    = testing(splits),
        actual_data = bind_rows(training(splits), testing(splits)),
        conf_by_id  = TRUE
    ) 
result1
```
```{r}
result1 %>%
    group_by(client) %>%
    filter(client == c( "1", "5", "7","10", "61","100"))%>%
    plot_modeltime_forecast(
        .facet_ncol  = 3,
        .interactive = FALSE,
        .title = "Forecast Plot",
        .line_alpha = 0.6,
        .line_size = 1,
        .y_intercept = 3.0
    )
```


#plot sens, spec, accuracy, j_index
```{r}
actual <- clients %>%
  select(date, client, shrunk_ae)
```

```{r}
predresult0 <- result0 %>%
  select(-.model_desc, -.conf_lo, -.conf_hi, -.key) %>%
  rename(model = .model_id, value = .value, date= .index)%>%
  relocate(model, value, .after = client)%>%
  pivot_wider(names_from = model, values_from =value)%>%
  rename(actual = "NA", windows = "1" )%>%
  drop_na()%>%
  mutate(obs =  ifelse(actual > 2.5, TRUE, FALSE))%>%
  mutate(windows_pred=  ifelse(windows > 2.5,TRUE, FALSE))%>%
  mutate(obs = as.factor(obs), windows_pred = as.factor(windows_pred))

conf0<-predresult0%>%
  group_by(date) %>%
  summarize(sens_windows = sens_vec(obs, windows_pred),
            spec_windows = spec_vec(obs, windows_pred), 
            jinex_window = j_index_vec(obs,windows_pred),
            acc_windows = accuracy_vec(obs, windows_pred))

conf0%>%
  pivot_longer(sens_windows, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
  
conf0%>%
  pivot_longer(spec_windows, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()

conf0%>%
  pivot_longer(jinex_window, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()

conf0%>%
  pivot_longer(acc_windows, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()

```


#plot sens, spec, accuracy, j_index



```{r}
threshold = 2.5
```

```{r}
predresult <- result %>%
  select(-.model_desc, -.conf_lo, -.conf_hi, -.key) %>%
  rename(model = .model_id, value = .value, date= .index)%>%
  relocate(model, value, .after = client)%>%
  pivot_wider(names_from = model, values_from =value)%>%
  rename(actual = "NA", rf = "1", rf_tuned = "2", svm_rbd = "3", knn = "4", xgboost = "5" )%>%
  drop_na()%>%
  mutate(obs =  ifelse(actual > threshold  , TRUE, FALSE))%>%
  mutate(rf_pred=  ifelse(rf > threshold ,TRUE, FALSE))%>%
  mutate(rftuned_pred=  ifelse(rf_tuned > threshold ,TRUE, FALSE))%>%
  mutate(svm_pred=  ifelse(svm_rbd > threshold ,TRUE, FALSE))%>%
  mutate(knn_pred=  ifelse(knn > threshold ,TRUE, FALSE))%>%
  mutate(xgboost_pred=  ifelse(xgboost > threshold ,TRUE, FALSE))%>%
  mutate(obs = as.factor(obs), rf_pred = as.factor(rf_pred), rftuned_pred = as.factor(rftuned_pred), 
         svm_pred = as.factor(svm_pred),knn_pred=as.factor(knn_pred), xgboost_pred = as.factor(xgboost_pred))

conf<-predresult%>%
  group_by(date) %>%
  summarize(sens_rf = sens_vec(obs, rf_pred), 
            sens_rftuned = sens_vec(obs, rftuned_pred),
            sens_svm = sens_vec(obs, svm_pred),
            sens_knn = sens_vec(obs, knn_pred),
            sens_xgboost = sens_vec(obs, xgboost_pred),
            spec_rf = spec_vec(obs, rf_pred), 
            spec_rftuned = spec_vec(obs, rftuned_pred),
            spec_svm = spec_vec(obs, svm_pred),
            spec_knn = spec_vec(obs, knn_pred),
            spec_xgboost = spec_vec(obs, xgboost_pred),
            jindex_rf = j_index_vec(obs, rf_pred),
            jindex_rftuned = j_index_vec(obs, rftuned_pred),
            jindex_svm = j_index_vec(obs, svm_pred),
            jindex_knn = j_index_vec(obs, knn_pred),
            jindex_xgboost = j_index_vec(obs, xgboost_pred),
            acc_rf = accuracy_vec(obs, rf_pred), 
            acc_rftuned = accuracy_vec(obs, rftuned_pred),
            acc_svm = accuracy_vec(obs, svm_pred),
            acc_knn = accuracy_vec(obs, knn_pred),
            acc_xgboost = accuracy_vec(obs, xgboost_pred))

conf%>%
  pivot_longer(sens_rf:sens_xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
  
conf%>%
  pivot_longer(spec_rf:spec_xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()

conf%>%
  pivot_longer(jindex_rf:jindex_xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()

conf%>%
  pivot_longer(acc_rf:acc_xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()

```

```{r}
predresult1 <- result1 %>%
  select(-.model_desc, -.conf_lo, -.conf_hi, -.key) %>%
  rename(model = .model_id, value = .value, date= .index)%>%
  relocate(model, value, .after = client)%>%
  pivot_wider(names_from = model, values_from =value)%>%
  rename(actual = "NA", rf = "1", rf_tuned = "2", svm_rbd = "3", knn = "4", xgboost = "5" )%>%
  drop_na()%>%
  mutate(obs =  ifelse(actual > threshold , TRUE, FALSE))%>%
  mutate(rf_pred=  ifelse(rf > threshold ,TRUE, FALSE))%>%
  mutate(rftuned_pred=  ifelse(rf_tuned > threshold ,TRUE, FALSE))%>%
  mutate(svm_pred=  ifelse(svm_rbd > threshold ,TRUE, FALSE))%>%
  mutate(knn_pred=  ifelse(knn > threshold ,TRUE, FALSE))%>%
  mutate(xgboost_pred=  ifelse(xgboost > threshold ,TRUE, FALSE))%>%
  mutate(obs = as.factor(obs), rf_pred = as.factor(rf_pred), rftuned_pred = as.factor(rftuned_pred), 
         svm_pred = as.factor(svm_pred),knn_pred=as.factor(knn_pred), xgboost_pred = as.factor(xgboost_pred))

conf1<-predresult1%>%
  group_by(date) %>%
  summarize(sens_rf = sens_vec(obs, rf_pred), sens_rftuned = sens_vec(obs, rftuned_pred),
            sens_svm = sens_vec(obs, svm_pred),sens_knn = sens_vec(obs, knn_pred),
            sens_xgboost = sens_vec(obs, xgboost_pred),
            spec_rf = spec_vec(obs, rf_pred), spec_rftuned = spec_vec(obs, rftuned_pred),
            spec_svm = spec_vec(obs, svm_pred),spec_knn = spec_vec(obs, knn_pred),
            spec_xgboost = spec_vec(obs, xgboost_pred),
            jindex_rf = j_index_vec(obs, rf_pred), jindex_rftuned = j_index_vec(obs, rftuned_pred),
            jindex_svm = j_index_vec(obs, svm_pred),jindex_knn = j_index_vec(obs, knn_pred),
            jindex_xgboost = j_index_vec(obs, xgboost_pred),
            acc_rf = accuracy_vec(obs, rf_pred), acc_rftuned = accuracy_vec(obs, rftuned_pred),
            acc_svm = accuracy_vec(obs, svm_pred),acc_knn = accuracy_vec(obs, knn_pred),
            acc_xgboost = accuracy_vec(obs, xgboost_pred))

conf1%>%
  pivot_longer(sens_rf:sens_xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
  
conf1%>%
  pivot_longer(spec_rf:spec_xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()

conf1%>%
  pivot_longer(jindex_rf:jindex_xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()

conf1%>%
  pivot_longer(acc_rf:acc_xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()

```


What to calculate claim
```{r}
claim <- read_feather("process.feather")%>%
  select(date, client, claims, expected, shrinkage,volume)
```
```{r}
predclaim <-
predresult %>%
  inner_join(claim, by = c("date", "client"))%>%
  mutate(rf = rf/shrinkage *(expected /52.18),
         rf_tuned = rf_tuned/shrinkage*(expected /52.18),
         svm_rbd = svm_rbd /shrinkage*(expected /52.18),
         knn = knn/shrinkage*(expected /52.18),
         xgboost= xgboost/shrinkage*(expected / 52.18))%>%
  select(date, client, claims,expected, rf, rf_tuned,svm_rbd,knn,xgboost)

```
```{r}
predclaim%>%
  group_by(date)%>%
  summarise(expected = sum(expected)/52.18,
        claims = sum(claims),
          rf =sum(rf) ,
         rf_tuned = sum(rf_tuned),
         svm_rbd = sum(svm_rbd) ,
         knn = sum(knn),
         xgboost= sum(xgboost))%>%
  pivot_longer(expected:xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```

```{r}
predclaim1 <-
predresult1 %>%
  inner_join(claim, by = c("date", "client"))%>%
  mutate(rf = rf/shrinkage *(expected /52.18),
         rf_tuned = rf_tuned/shrinkage*(expected /52.18),
         svm_rbd = svm_rbd /shrinkage*(expected /52.18),
         knn = knn/shrinkage*(expected /52.18),
         xgboost= xgboost/shrinkage*(expected / 52.18))%>%
  select(date, client, claims,expected, rf, rf_tuned,svm_rbd,knn,xgboost)

```
```{r}
predclaim1%>%
  group_by(date)%>%
  summarise(expected = sum(expected)/52.18,
        claims = sum(claims),
          rf =sum(rf) ,
         rf_tuned = sum(rf_tuned),
         svm_rbd = sum(svm_rbd) ,
         knn = sum(knn),
         xgboost= sum(xgboost))%>%
  pivot_longer(expected:xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = date, y = value, color = metric)) + geom_line()
```
```{r}
predclaim%>%
  group_by(client)%>%
  
  summarise(expected = sum(expected)/52.18,
        claims = sum(claims),
          rf =sum(rf) ,
         rf_tuned = sum(rf_tuned),
         svm_rbd = sum(svm_rbd) ,
         knn = sum(knn),
         xgboost= sum(xgboost))%>%
  pivot_longer(claims:xgboost, names_to = "metric", values_to = "value")%>%
  ggplot(aes(x = client, y = value, color = metric)) + geom_line()
```


